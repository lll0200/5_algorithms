C45
C4.5是决策树算法的一种。决策树算法作为一种分类算法，目标就是将具有p维特征的n个样本分到c个类别中去。
算法流程：
1.计算类别信息熵
2.计算每个属性的信息熵
3.计算信息增益
4.计算属性分裂信息度量
5.计算信息增益率
优点
产生的分类规则易于理解，准确率较高。
缺点
在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。

CART
CART与C4.5类似，是决策树算法的一种。此外，常见的决策树算法还有ID3，这三者的不同之处在于特征的划分：
ID3：特征划分基于信息增益
C4.5：特征划分基于信息增益比
CART：特征划分基于基尼指数
CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

CART算法由以下两步组成：
决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；
决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。
CART决策树的生成就是递归地构建二叉决策树的过程。CART决策树既可以用于分类也可以用于回归。本文我们仅讨论用于分类的CART。对分类树而言，CART用GINI系数最小化准则来进行特征选择，生成二叉树。 CART生成算法如下：
输入：训练数据集D，停止计算的条件：
输出：CART决策树。
根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：
设结点的训练数据集为D，计算现有特征对该数据集的GINI系数。此时，对每一个特征A，对其可能取的每个值a，根据样本点对A=a的测试为“是”或 “否”将D分割成D1和D2两部分，计算A=a时的GINI系数。
在所有可能的特征A以及它们所有可能的切分点a中，选择GINI系数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。
对两个子结点递归地调用步骤l~2，直至满足停止条件。
生成CART决策树。
算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的GINI系数小于预定阈值（样本基本属于同一类），或者没有更多特征。

ID3
ID3 算法是澳洲计算机科学家Ross QUINLAN发明的，全称是Iterative DICHOTOMISER3。
ID3 算法的作用是通过一个数据集来生成一棵决策树。
ID3 算法的主要应用领域有：1，机器学习，2，自然语言处理。
ID3 算法的执行流程：
第一步是递归地构建决策树，计算信息增益最大（或者熵最小）的特征作为最优特征。
递归的出口是：1，所有标签都一样。 2，当所有特征都遍历过，那么选择出现最多的标签。

KNN
又叫K-邻近算法，是监督学习中的一种分类算法。目的是根据已知类别的样本点集求出待分类的数据点类别。
KNN是一种lazy-learning算法，分类器不需要使用训练集进行训练，因此训练时间复杂度为0；KNN分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为n，那么KNN的分类时间复杂度为O(n)；因此，最终的时间复杂度是O(n)。
优点
理论成熟，思想简单，既可以用来做分类也可以用来做回归 ；
适合对稀有事件进行分类（例如：客户流失预测）；
特别适合于多分类问题(multi-modal,对象具有多个类别标签，例如：根据基因特征来判断其功能分类)， KNN比SVM的表现要好。
缺点
当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数；
计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点；
可理解性差，无法给出像决策树那样的规则。

NAVIVEBAYS
NAVIVEBAYS算法，又叫朴素贝叶斯算法，朴素：特征条件独立；贝叶斯：基于贝叶斯定理。属于监督学习的生成模型，实现简单，没有迭代，并有坚实的数学理论（即贝叶斯定理）作为支撑。在大量样本下会有较好的表现，不适用于输入向量的特征条件有关联的场景。

朴素贝叶斯常用的三个模型有：
高斯模型：处理特征是连续型变量的情况
多项式模型：最常见，要求特征是离散数据
伯努利模型：要求特征是离散的，且为布尔类型，即true和false，或者1和0